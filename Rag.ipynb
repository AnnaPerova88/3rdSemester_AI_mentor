{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4ZNOu8qHTwz",
        "outputId": "34cd1d07-20b1-4eaa-a14b-2e2de4d25f8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7f9d5e731440>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jax/_src/lib/__init__.py\", line 127, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio sentence-transformers faiss-cpu requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBiAXEseLDYm"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import requests, re\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I4sWCBhLPb3"
      },
      "outputs": [],
      "source": [
        "SOURCES = {\n",
        "    \"Machine Learning\": [\n",
        "        (\"Goodfellow ML\", \"https://www.deeplearningbook.org/contents/ml.html\"),\n",
        "        (\"Andrew Ng ML (Notes)\", \"https://cs229.stanford.edu/\"),\n",
        "        (\"MIT Intro to ML\", \"https://introtoml.mit.edu/\"),\n",
        "        (\"Google ML Crash Course\", \"https://developers.google.com/machine-learning/crash-course\")\n",
        "    ],\n",
        "\n",
        "    \"Computer Vision\": [\n",
        "        (\"CS231n Stanford\", \"http://cs231n.stanford.edu/\"),\n",
        "        (\"PyImageSearch CV\", \"https://www.pyimagesearch.com/start-here/\"),\n",
        "        (\"OpenCV Tutorials\", \"https://docs.opencv.org/4.x/d9/df8/tutorial_root.html\"),\n",
        "        (\"Learn PyTorch CV\", \"https://www.learnpytorch.io/\")\n",
        "    ],\n",
        "\n",
        "    \"NLP\": [\n",
        "        (\"CS224n Stanford\", \"https://cs224n.stanford.edu/\"),\n",
        "        (\"Jurafsky NLP Book\", \"https://web.stanford.edu/~jurafsky/slp3/\"),\n",
        "        (\"HuggingFace NLP Course\", \"https://huggingface.co/learn/nlp-course\"),\n",
        "        (\"NLTK Book\", \"https://www.nltk.org/book/\")\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-3fLpvYLfwn"
      },
      "outputs": [],
      "source": [
        "# ---------------- –ó–ê–ì–†–£–ó–ö–ê –¢–ï–ö–°–¢–û–í (–£–õ–£–ß–®–ï–ù–ù–ê–Ø) ----------------\n",
        "def load_docs():\n",
        "    docs = []\n",
        "\n",
        "    for topic, items in SOURCES.items():\n",
        "        for name, url in items:\n",
        "            try:\n",
        "                r = requests.get(\n",
        "                    url,\n",
        "                    timeout=15,\n",
        "                    headers={\"User-Agent\": \"Mozilla/5.0\"}\n",
        "                )\n",
        "                r.raise_for_status()\n",
        "\n",
        "                soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "                # –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —Ç–µ–≥–∏\n",
        "                for tag in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]):\n",
        "                    tag.decompose()\n",
        "\n",
        "                text = soup.get_text(separator=\" \")\n",
        "                text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ (–≤–∞–∂–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –ø–∞–º—è—Ç–∏)\n",
        "                text = text[:4000]\n",
        "\n",
        "            except Exception as e:\n",
        "                # fallback, —á—Ç–æ–±—ã RAG –Ω–µ –ª–æ–º–∞–ª—Å—è\n",
        "                text = f\"Educational material about {name} ({topic})\"\n",
        "\n",
        "            docs.append({\n",
        "                \"topic\": topic,\n",
        "                \"name\": name,\n",
        "                \"url\": url,\n",
        "                \"text\": text\n",
        "            })\n",
        "\n",
        "    return docs\n",
        "\n",
        "\n",
        "docs = load_docs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xydlvv2_LjiP"
      },
      "outputs": [],
      "source": [
        "# ---------------- –ß–ê–ù–ö–ò–ù–ì ----------------\n",
        "def chunk_text(\n",
        "    text: str,\n",
        "    chunk_size: int = 600,\n",
        "    overlap: int = 120\n",
        "):\n",
        "    \"\"\"\n",
        "    –î–µ–ª–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º.\n",
        "    chunk_size –∏ overlap ‚Äî –≤ —Å–∏–º–≤–æ–ª–∞—Ö.\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    chunks = []\n",
        "\n",
        "    start = 0\n",
        "    text_len = len(text)\n",
        "\n",
        "    while start < text_len:\n",
        "        end = min(start + chunk_size, text_len)\n",
        "        chunk = text[start:end].strip()\n",
        "\n",
        "        # –§–∏–ª—å—Ç—Ä —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö —á–∞–Ω–∫–æ–≤\n",
        "        if len(chunk) >= 200:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        # —à–∞–≥ –Ω–∞–∑–∞–¥ –¥–ª—è overlap\n",
        "        start = end - overlap\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "\n",
        "        if end == text_len:\n",
        "            break\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# –ø—Ä–∏–º–µ–Ω—è–µ–º –∫–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\n",
        "chunks = []\n",
        "for d in docs:\n",
        "    text_chunks = chunk_text(d[\"text\"])\n",
        "    for ch in text_chunks:\n",
        "        chunks.append({\n",
        "            \"topic\": d[\"topic\"],\n",
        "            \"name\": d[\"name\"],\n",
        "            \"url\": d[\"url\"],\n",
        "            \"text\": ch\n",
        "        })\n",
        "\n",
        "print(f\"‚úÖ –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHGqQBxSLsmP"
      },
      "outputs": [],
      "source": [
        "# ---------------- –ú–û–î–ï–õ–¨ + FAISS ----------------\n",
        "\n",
        "# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ª—ë–≥–∫–∞—è –∏ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è)\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 2. –ö–æ–¥–∏—Ä—É–µ–º –≤—Å–µ —á–∞–Ω–∫–∏\n",
        "texts = [c[\"text\"] for c in chunks]\n",
        "embeddings = model.encode(\n",
        "    texts,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "# 3. FAISS –∏–Ω–¥–µ–∫—Å (cosine similarity —á–µ—Ä–µ–∑ L2 + normalization)\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings.astype(\"float32\"))\n",
        "\n",
        "print(f\"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω | —á–∞–Ω–∫–æ–≤: {len(chunks)} | dim: {dimension}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHnXkyK0L3Ud"
      },
      "outputs": [],
      "source": [
        "# ---------------- RAG –ü–û–ò–°–ö ----------------\n",
        "def rag_search(query: str, k: int = 3):\n",
        "    \"\"\"\n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç top-k –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤\n",
        "    \"\"\"\n",
        "    q_emb = model.encode(\n",
        "        [query],\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "\n",
        "    distances, indices = index.search(q_emb.astype(\"float32\"), k)\n",
        "\n",
        "    results = []\n",
        "    for rank, idx in enumerate(indices[0]):\n",
        "        if idx < len(chunks):\n",
        "            results.append({\n",
        "                \"rank\": rank + 1,\n",
        "                \"score\": float(distances[0][rank]),\n",
        "                \"topic\": chunks[idx][\"topic\"],\n",
        "                \"name\": chunks[idx][\"name\"],\n",
        "                \"url\": chunks[idx][\"url\"],\n",
        "                \"text\": chunks[idx][\"text\"]\n",
        "            })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWZh9AZ2L6vj"
      },
      "outputs": [],
      "source": [
        "# ---------------- –õ–û–ì–ò–ö–ê UI ----------------\n",
        "def get_lesson(topic, level, query):\n",
        "    results = rag_search(f\"{query} {topic} {level}\")\n",
        "\n",
        "    # –ó–∞–≥–æ–ª–æ–≤–æ–∫\n",
        "    out = f\"## üß† {query}\\n\"\n",
        "    out += f\"**–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ:** {topic}  \\n\"\n",
        "    out += f\"**–£—Ä–æ–≤–µ–Ω—å:** {level}\\n\\n\"\n",
        "    out += \"---\\n\\n\"\n",
        "\n",
        "    # –ë–ª–æ–∫ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏\n",
        "    out += \"### üìö –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\\n\\n\"\n",
        "\n",
        "    for r in results:\n",
        "        out += f\"**{r['rank']}. {r['name']}**  \\n\"\n",
        "        out += f\"*Relevance score:* `{r['score']:.3f}`  \\n\\n\"\n",
        "        out += f\"{r['text'][:400]}...\\n\\n\"\n",
        "        out += f\"üîó [{r['url']}]({r['url']})\\n\\n\"\n",
        "        out += \"---\\n\\n\"\n",
        "\n",
        "    # –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å\n",
        "    out += \"### ‚ùì –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å\\n\"\n",
        "    out += (\n",
        "        f\"–ß—Ç–æ —Ç–∞–∫–æ–µ **{query.lower()}** –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ **{topic}**, \"\n",
        "        f\"–∏ –∫–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ —É—Ä–æ–≤–Ω–µ **{level}**?\"\n",
        "    )\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbimT71pa-8l"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "oUjQ-x2HOkXa",
        "outputId": "e00d7a35-cf9d-48b5-8bc6-800308eba504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e56851397d38837800.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://e56851397d38837800.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ================== RAG + GRADIO (SAFE MODE) ==================\n",
        "\n",
        "import gradio as gr\n",
        "import requests, re\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# ---------- 1. –ò–°–¢–û–ß–ù–ò–ö–ò ----------\n",
        "SOURCES = {\n",
        "    \"Machine Learning\": [\n",
        "        (\"Goodfellow ML\", \"https://www.deeplearningbook.org/contents/ml.html\"),\n",
        "        (\"Andrew Ng ML\", \"https://www.coursera.org/learn/machine-learning\")\n",
        "    ],\n",
        "    \"Computer Vision\": [\n",
        "        (\"CS231n\", \"http://cs231n.stanford.edu/\")\n",
        "    ],\n",
        "    \"NLP\": [\n",
        "        (\"CS224n\", \"https://cs224n.stanford.edu/\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "# ---------- 2. –ó–ê–ì–†–£–ó–ö–ê –¢–ï–ö–°–¢–û–í ----------\n",
        "def load_docs():\n",
        "    docs = []\n",
        "    for topic, items in SOURCES.items():\n",
        "        for name, url in items:\n",
        "            try:\n",
        "                r = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "                soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "                text = soup.get_text(\" \")\n",
        "                text = re.sub(r\"\\s+\", \" \", text)[:4000]\n",
        "            except:\n",
        "                text = \"\"\n",
        "            docs.append({\n",
        "                \"topic\": topic,\n",
        "                \"name\": name,\n",
        "                \"url\": url,\n",
        "                \"text\": text\n",
        "            })\n",
        "    return docs\n",
        "\n",
        "docs = load_docs()\n",
        "\n",
        "# ---------- 3. –ß–ê–ù–ö–ò ----------\n",
        "chunks = []\n",
        "for d in docs:\n",
        "    for i in range(4):\n",
        "        chunk_text = d[\"text\"][i*500:(i+1)*500]\n",
        "        if len(chunk_text) > 100:\n",
        "            chunks.append({\n",
        "                \"topic\": d[\"topic\"],\n",
        "                \"name\": d[\"name\"],\n",
        "                \"url\": d[\"url\"],\n",
        "                \"text\": chunk_text\n",
        "            })\n",
        "\n",
        "# ---------- 4. FAISS ----------\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode([c[\"text\"] for c in chunks])\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings.astype(\"float32\"))\n",
        "\n",
        "def rag_search(query, k=3):\n",
        "    q = model.encode([query])\n",
        "    _, idx = index.search(q.astype(\"float32\"), k)\n",
        "    return [chunks[i] for i in idx[0]]\n",
        "\n",
        "# ---------- 5. –õ–û–ì–ò–ö–ê –í–´–í–û–î–ê (–ë–ï–ó LLM) ----------\n",
        "def get_rag_answer(topic, level, query):\n",
        "    results = rag_search(f\"{query} {topic}\")\n",
        "\n",
        "    if not results:\n",
        "        return \"‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\"\n",
        "\n",
        "    out = f\"## üìö RAG-—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\\n\"\n",
        "    out += f\"**–¢–µ–º–∞:** {query}\\n\"\n",
        "    out += f\"**–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ:** {topic}\\n\"\n",
        "    out += f\"**–£—Ä–æ–≤–µ–Ω—å:** {level}\\n\\n\"\n",
        "\n",
        "    for r in results:\n",
        "        out += f\"### üîπ {r['name']}\\n\"\n",
        "        out += f\"{r['text'][:300]}...\\n\\n\"\n",
        "        out += f\"üîó {r['url']}\\n\\n---\\n\"\n",
        "\n",
        "    return out\n",
        "\n",
        "# ---------- 6. GRADIO UI ----------\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# üß† RAG-—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä (–±–µ–∑ LLM, —Å —Ü–∏—Ç–∞—Ç–∞–º–∏)\")\n",
        "    gr.Markdown(\"–°—Ç–∞–±–∏–ª—å–Ω—ã–π —Ä–µ–∂–∏–º: –ø–æ–∏—Å–∫ –ø–æ —É—á–µ–±–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º.\")\n",
        "\n",
        "    topic = gr.Radio(\n",
        "        [\"Machine Learning\", \"Computer Vision\", \"NLP\"],\n",
        "        label=\"üìö –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ\",\n",
        "        value=\"Machine Learning\"\n",
        "    )\n",
        "\n",
        "    level = gr.Radio(\n",
        "        [\"Beginner\", \"Middle\", \"Senior\"],\n",
        "        label=\"üéì –£—Ä–æ–≤–µ–Ω—å\",\n",
        "        value=\"Beginner\"\n",
        "    )\n",
        "\n",
        "    query = gr.Textbox(\n",
        "        label=\"–¢–µ–º–∞\",\n",
        "        value=\"Overfitting\"\n",
        "    )\n",
        "\n",
        "    btn = gr.Button(\"üîç –ù–∞–π—Ç–∏ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö\")\n",
        "    output = gr.Markdown()\n",
        "\n",
        "    btn.click(\n",
        "        fn=get_rag_answer,\n",
        "        inputs=[topic, level, query],\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)\n",
        "\n",
        "# ============================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn6TqorAdXqV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}